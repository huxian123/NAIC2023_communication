{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun  1 10:25:26 2023\n",
    "\n",
    "@author: ZLi27\n",
    "\"\"\"\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import scipy.io as scio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "Rx_num = 4\n",
    "Tx_num = 32\n",
    "SC_num = 128\n",
    "\n",
    "def DownPrecoding(channel_est):  ### Optional\n",
    "    ### estimated channel\n",
    "    HH_est = np.reshape(channel_est, (-1, Rx_num, Tx_num, SC_num, 2)) ## Rx, Tx, Subcarrier, RealImag\n",
    "    HH_complex_est = HH_est[:,:,:,:,0] + 1j * HH_est[:,:,:,:,1]  ## Rx, Tx, Subcarrier\n",
    "    HH_complex_est = np.transpose(HH_complex_est, [0,3,1,2])\n",
    "\n",
    "    ### precoding based on the estimated channel\n",
    "    _, _, MatTx = np.linalg.svd(HH_complex_est, full_matrices=False) ## SVD\n",
    "    PrecodingVector = np.conj(MatTx[:,:,0,:])  ## The best eigenvector (MRT transmission)\n",
    "    PrecodingVector = np.reshape(PrecodingVector,(-1, SC_num, Tx_num, 1))\n",
    "    return PrecodingVector\n",
    "\n",
    "def EqChannelGain(channel, PrecodingVector):\n",
    "    ### The authentic CSI\n",
    "    HH = np.reshape(channel, (-1, Rx_num, Tx_num, SC_num, 2)) ## Rx, Tx, Subcarrier, RealImag\n",
    "    HH_complex = HH[:,:,:,:,0] + 1j * HH[:,:,:,:,1]  ## Rx, Tx, Subcarrier\n",
    "    HH_complex = np.transpose(HH_complex, [0,3,1,2])\n",
    "\n",
    "    ### Power Normalization of the precoding vector\n",
    "    Power = np.matmul(np.transpose(np.conj(PrecodingVector), (0, 1, 3, 2)), PrecodingVector)\n",
    "    PrecodingVector = PrecodingVector/ np.sqrt(Power)\n",
    "\n",
    "    ### Effective channel gain\n",
    "    R = np.matmul(HH_complex, PrecodingVector)\n",
    "    R_conj = np.transpose(np.conj(R), (0, 1, 3, 2))\n",
    "    h_sub_gain =  np.matmul(R_conj, R)\n",
    "    h_sub_gain = np.reshape(np.absolute(h_sub_gain), (-1, SC_num))  ### channel gain of SC_num subcarriers\n",
    "    return h_sub_gain\n",
    "\n",
    "def DataRate(h_sub_gain, sigma2_UE):  ### Score\n",
    "    SNR = h_sub_gain / sigma2_UE\n",
    "    Rate = np.log2(1 + SNR)  ## rate\n",
    "    Rate_OFDM = torch.mean(Rate, axis=-1)  ###  averaging over subcarriers\n",
    "    Rate_OFDM_mean = torch.mean(Rate_OFDM)  ### averaging over CSI samples\n",
    "    return Rate_OFDM_mean\n",
    "\n",
    "\n",
    "##############################################Resnet9D################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.mish1 = Mish()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.mish2 = Mish()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.mish1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.mish2(out)\n",
    "        return out\n",
    "\n",
    "class ResNet9D(nn.Module):\n",
    "    def __init__(self, input_dim=3,num_classes=2):\n",
    "        super(ResNet9D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.mish1 = Mish()\n",
    "        self.layer1 = self._make_layer(64, 32, 1)\n",
    "        self.layer2 = self._make_layer(32, 128, 2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        out = self.mish1(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool1d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = out.view(-1,4,32,128,2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ANN_TypeI(nn.Module):\n",
    "    def __init__(self,input_dim=3):\n",
    "        super().__init__()\n",
    "        self.res = ResNet9D(input_dim=input_dim, num_classes=4*32*128*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.res(x)\n",
    "        return out\n",
    "\n",
    "class RadioMap_Model_TypeI(nn.Module): ### Generate RadioMapI (Input:location, Output:beamforming vector)\n",
    "    def __init__(self, input_dim=3):\n",
    "        super(RadioMap_Model_TypeI, self).__init__()\n",
    "        self.ann = ANN_TypeI(input_dim) ## Neural Network (input:location, output: the estimated CSI)\n",
    "\n",
    "    def forward(self, p):\n",
    "        CSI_est = self.ann(p)\n",
    "        # HH_est = torch.reshape(CSI_est, [-1, Rx_num, Tx_num, SC_num, 2]) ## the estimated CSI, shape: Rx, Tx, Subcarrier, RealImag\n",
    "        # HH_complex_est = torch.complex(HH_est[:,:,:,:,0], HH_est[:,:,:,:,1])\n",
    "        # HH_complex_est = HH_complex_est.permute(0,3,1,2)\n",
    "        # _, _, MatTx = torch.linalg.svd(HH_complex_est, full_matrices=True) ## Note that tf.svd is different from np.svd\n",
    "        # PrecodingVector = MatTx[:,:,:,0] ## The best eigenvector (MRT transmission)\n",
    "        PrecodingVector = torch.tensor(DownPrecoding(CSI_est.cpu()))\n",
    "        # PrecodingVector = torch.reshape(PrecodingVector,[-1, SC_num, Tx_num, 1])\n",
    "        return PrecodingVector\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    # Get cpu, gpu or mps device for training.\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using {device} device\")\n",
    "    return device\n",
    "\n",
    "def get_dataset_iter(features, label, device, batch_size=128, shuffle=True, validation_split=0.1):\n",
    "    size = len(features)\n",
    "    val_len = int(size*validation_split)\n",
    "    lengths = [size-val_len, val_len]\n",
    "    features_tensor = torch.tensor(features, device=device)\n",
    "    label_tensor = torch.tensor(label, device=device)\n",
    "    dataset = Data.TensorDataset(features_tensor, label_tensor)\n",
    "    train_dataset, val_dataset = Data.random_split(dataset, lengths=lengths, generator=torch.Generator().manual_seed(42))\n",
    "    train_dataset_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataset_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataset_iter, val_dataset_iter\n",
    "\n",
    "def get_tensor(features, device):\n",
    "    features_tensor = torch.tensor(features, device=device)\n",
    "    return features_tensor\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred.to(torch.float64), y.to(torch.float64))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def verify(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dataloader):\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred.to(torch.float64), y.to(torch.float64)).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "def inference(tensor, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor)\n",
    "    return pred.cpu().numpy()\n",
    "\n",
    "\n",
    "## device\n",
    "device = get_device()\n",
    "\n",
    "## Parameters\n",
    "SC_num = 128  ## subcarrier number\n",
    "Tx_num = 32 ## Tx antenna number\n",
    "Rx_num = 4  ## Rx antenna number\n",
    "sigma2_UE =  0.1\n",
    "\n",
    "#### Read Data\n",
    "f = scio.loadmat('./train.mat')\n",
    "data = f['train']\n",
    "location_data = data[0,0]['loc']\n",
    "channel_data = data[0,0]['CSI']\n",
    "print(location_data.shape)\n",
    "print(channel_data.shape)\n",
    "## basic train parameters\n",
    "loss_fn = nn.MSELoss()\n",
    "EPOCHS = 1000\n",
    "LR = 5e-4\n",
    "batch_size=128\n",
    "\n",
    "########################################################   Scheme 1   ########################################################\n",
    "## ANN for CSI estimation\n",
    "train_dataset_iter, val_dataset_iter = get_dataset_iter(location_data, channel_data, device)\n",
    "\n",
    "# from VIT import ANN_TypeI\n",
    "input_dim = 3\n",
    "net_type1 = ANN_TypeI(input_dim).to(device)\n",
    "opt_adam_type1 = torch.optim.Adam(net_type1.parameters(),lr=LR)\n",
    "best_loss = 0\n",
    "test_losses = []\n",
    "RadioMap_TypeI = RadioMap_Model_TypeI(input_dim=input_dim)\n",
    "RadioMap_TypeI.to(device)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train(train_dataset_iter, net_type1, loss_fn, opt_adam_type1)\n",
    "    test_loss = verify(val_dataset_iter, net_type1, loss_fn)\n",
    "    if test_loss > best_loss:\n",
    "        best_loss = test_loss\n",
    "        RadioMap_TypeI.ann=net_type1\n",
    "        torch.save(RadioMap_TypeI.state_dict(), \"./model.pth\")\n",
    "        print(\"Saved PyTorch Model State to model.pth\")\n",
    "        # torch.save(net_type1.state_dict(), f'csi_pre.pth')\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"the best test loss is {max(test_losses)}, epoch {test_losses.index(max(test_losses))}\")\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
